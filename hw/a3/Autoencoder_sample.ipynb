{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder_sample.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35Tv2peftFL7"
      },
      "source": [
        "This notebook uses [PyTorch](https://pytorch.org/), which is a library that can automatically differentiate functions and is commonly used to implement neural networks. Because of its capability to automatically differentiate functions, the formula for the gradient does not need to be manually derived -- this comes especially handy when working with neural networks, whose gradient formula can be quite complicated, especially for complex architectures. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inyJ_O0zvGjG"
      },
      "source": [
        "We recommend going over the [PyTorch tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html) and then the starter code below. For any new functions that you come across, you can look up the documentation [here](https://pytorch.org/docs/stable/index.html). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIzZYEqUwQKw"
      },
      "source": [
        "We recommend running this notebook on Google Colab to avoid having to install PyTorch and to take advantage of GPUs, which make training faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyJgJ5DNERo5"
      },
      "source": [
        "## Load packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T21:12:35.634207Z",
          "start_time": "2021-03-11T21:12:34.239216Z"
        },
        "id": "mQ0w94zPtMvw"
      },
      "source": [
        "# load packages\n",
        "# make sure to install the pacakge \"tqdm\" for the progress bar when training.\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import matplotlib.image as mpimg\n",
        "from scipy import ndimage\n",
        "\n",
        "path_prefix = \"\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9NjnIpIa5nz"
      },
      "source": [
        "If running on Google Colab, you need to upload the file `autoencoder_starter.py`. To do so, you need to click on the folder icon on the left side of the page, which brings up a panel that would allow you to upload files. Note however uploading the file this way has a downside in that if your Python runtime times out or is otherwise restarted, the file will be erased and you will need to re-upload. \n",
        "\n",
        "If you would like to avoid this, you can create a directory named `CMPT_726Fall2021_A3` in your Google Drive and upload `autoencoder_starter.py` to that directory. Then execute the following block of code to mount your Google Drive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwJsQVpTuLRE"
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "trainTransform  = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
        "trainset = torchvision.datasets.FashionMNIST(root='{}/./data'.format(path_prefix),  train=True,download=True, transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD9n2s211Rii"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "path_prefix = \"/content/gdrive/MyDrive/CMPT_726Fall2021_A3\"\n",
        "\n",
        "import sys\n",
        "sys.path.insert(1, path_prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM0qdo6PEW1d"
      },
      "source": [
        "## Define your architecture here.\n",
        "\n",
        "The `Autoencoder` class has several important functions unimplemented. You are required to implement the two sub-classes of `Encoder` and `Decoder`, i.e, the architecture and forward function of the encoder and decoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T21:12:35.656496Z",
          "start_time": "2021-03-11T21:12:35.637352Z"
        },
        "id": "23IwboWcDwBA"
      },
      "source": [
        "class Autoencoder(nn.Module):\n",
        "\n",
        "    def __init__(self,dim_latent_representation=2):\n",
        "\n",
        "        super(Autoencoder,self).__init__()\n",
        "\n",
        "        class Encoder(nn.Module):\n",
        "            def __init__(self, output_size=2):\n",
        "                super(Encoder, self).__init__()\n",
        "                # needs your implementation\n",
        "                pass\n",
        "\n",
        "            def forward(self, x):\n",
        "                # needs your implementation\n",
        "                pass\n",
        "\n",
        "        class Decoder(nn.Module):\n",
        "            def __init__(self, input_size=2):\n",
        "                super(Decoder, self).__init__()\n",
        "                # needs your implementation\n",
        "                pass\n",
        "\n",
        "            def forward(self, z):\n",
        "                # needs your implementation\n",
        "                pass\n",
        "\n",
        "        self.encoder = Encoder(output_size=dim_latent_representation)\n",
        "        self.decoder = Decoder(input_size=dim_latent_representation)\n",
        "\n",
        "    # Implement this function for the DAE model\n",
        "    # def add_noise(self, x, noise_type):\n",
        "    #     if noise_type=='Gaussian':\n",
        "    #         # return (x with Gaussian noise)\n",
        "    #     elif noise_type=='Dropout':\n",
        "    #         return (x with Dropout noise)\n",
        "\n",
        "    # Implement this function for the VAE model\n",
        "    # def reparameterise(self, mu, logvar):\n",
        "    #     if self.training:\n",
        "    #         # return reparametrized mu\n",
        "    #     else:\n",
        "    #         return mu\n",
        "\n",
        "    def forward(self,x):\n",
        "        # This function should be modified for the DAE and VAE\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        # for the VAE forward function should also return mu and logvar\n",
        "        return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfMcq9AHEabL"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "The training loop is provided by the `Autoencoder_Trainer` class from `autoencoder_starter.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-11T21:12:46.685307Z",
          "start_time": "2021-03-11T21:12:35.661256Z"
        },
        "id": "SqcNfjOTDz2j"
      },
      "source": [
        "from autoencoder_starter import Autoencoder_Trainer\n",
        "\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCH_NUMBER= 10 # the number of epochs and learning rate can be tuned.\n",
        "\n",
        "autoencoder = Autoencoder(dim_latent_representation=2)\n",
        "trainer = Autoencoder_Trainer(autoencoder_model=autoencoder,learning_rate=LEARNING_RATE,path_prefix=path_prefix)\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, EPOCH_NUMBER + 1):\n",
        "        trainer.train(epoch)\n",
        "        trainer.validate(epoch)\n",
        "except (KeyboardInterrupt, SystemExit):\n",
        "        print(\"Manual Interruption\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xZdZKnc0j1P"
      },
      "source": [
        "If in the above cell, you find that the download of the dataset is unsuccessful because the server is down, you can also download the data from Canvas and put the data in the same directory of this notebook and `autoencoder_starter.py`. If you are running this notebook on Google Colab, you can create a directory called `CMPT_726Fall2021_A3` in your Google Drive and put the data directory you downloaded from Canvas inside. Then if you haven't mounted your Google Drive earlier using the block of code above, mount Google Drive by executing the following block of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dCCuz2mbAo0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "path_prefix = \"/content/gdrive/MyDrive/CMPT_726Fall2021_A3\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1M7mOuREg1z"
      },
      "source": [
        "## Visualizing 2D Bottleneck Feature Representations\n",
        "\n",
        "We can visualize the 2D bottleneck representations of data points and their ground truth class labels with a scatter plot.\n",
        "\n",
        "The `scatter_plot` function takes the following arguments:\n",
        "* latent_presentations - (N, dimension_latent_representation) numpy array\n",
        "* labels - (N, ) numpy array: the labels of the ground truth classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-03-11T21:12:57.723Z"
        },
        "id": "0itXRhM8D2MB"
      },
      "source": [
        "with torch.no_grad():\n",
        "    model = trainer.model\n",
        "    model.eval()\n",
        "    z=[];label=[]\n",
        "    for x,y in trainer.val_loader:\n",
        "\n",
        "        z_ = model.encoder(x.to(trainer.device))\n",
        "        z += z_.cpu().tolist()\n",
        "        label += y.cpu().tolist()\n",
        "    z = np.asarray(z)\n",
        "    label = np.asarray(label)\n",
        "\n",
        "from autoencoder_starter import scatter_plot\n",
        "scatter_plot(latent_representations=z,labels=label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMzjgvmWujFp"
      },
      "source": [
        "## Generating Images by Sampling Bottleneck Features\n",
        "\n",
        "So here we can generate new images by sampling bottleneck features, and use the decoder to generate images. \n",
        "\n",
        "The code below provides an example of how to generate images by sampling bottleneck features. \n",
        "\n",
        "The `display_images_in_a_row` takes the following arguments:\n",
        "* images: (N,28,28): N images of 28*28 as a numpy array\n",
        "* file_path: file path name for where to store the figure\n",
        "* display: display the image or not\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-03-11T21:11:45.425Z"
        },
        "id": "ZPGfWcqusvrG"
      },
      "source": [
        "with torch.no_grad():\n",
        "    samples = torch.randn(7, 2).to(trainer.device)\n",
        "    samples = trainer.model.decoder(samples).cpu()\n",
        "\n",
        "images = samples\n",
        "\n",
        "from autoencoder_starter import display_images_in_a_row\n",
        "display_images_in_a_row(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkQ7IZXNEexN"
      },
      "source": [
        "## Reconstructing Images\n",
        "\n",
        "We can first retrieve the validation set and then pick 64 images (the first 64 images, though you can do it randomly).\n",
        "\n",
        "We use the autoencoder to reconstruct the images and visualize them below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2021-03-11T21:11:45.429Z"
        },
        "id": "6YmbPNYcSStS"
      },
      "source": [
        "images = trainer.get_val_set() # get the entire validation set\n",
        "total_number = 64\n",
        "images = images[:total_number]\n",
        "\n",
        "from autoencoder_starter import display_images_in_a_row\n",
        "print(\"Original images\")\n",
        "display_images_in_a_row(images.cpu())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_0iajk9GQL7"
      },
      "source": [
        "with torch.no_grad():\n",
        "    images = images.to(trainer.device)\n",
        "    reconstructed = trainer.model(images).cpu()\n",
        "print(\"Reconstructed images\")\n",
        "display_images_in_a_row(reconstructed)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}